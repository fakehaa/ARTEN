{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Translation From Arabic to English Using Spark NLP"
      ],
      "metadata": {
        "id": "Z_eodIn8AmND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook contains code of our implementation of SPARK NLP and PYSPARK for machine translation. The input language is Arabic whereas the target language is English."
      ],
      "metadata": {
        "id": "glgqGTUF3wql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xGW5PbU538bX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIeCOiJNW-88"
      },
      "source": [
        "## Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CGJktFHdHL1n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1f0f51f-4011-4efa-db74-0e703ef5f9f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.8/453.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# installing libraries for NLP\n",
        "! pip install -q pyspark==3.3.0 spark-nlp==4.2.8\n",
        "\n",
        "! pip install --upgrade -q spark-nlp-display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCIT5VLxS3I1"
      },
      "source": [
        "## Start the Spark session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khjM-z9ORFU3"
      },
      "source": [
        "Import dependencies and relevant modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sw-t1zxlHTB7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import sparknlp\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "from pyspark.sql.types import StringType, IntegerType"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark_instance = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP \", sparknlp.version())\n",
        "print(\"Apache Spark :\", spark_instance.version)\n",
        "\n",
        "spark_instance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "2PKMEfcevAsS",
        "outputId": "4dea969d-3777-4e67-a153-853ae9466a0d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP  4.2.8\n",
            "Apache Spark : 3.3.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7d8842e7c220>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://55381045491c:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y9GpdJhXIpD"
      },
      "source": [
        "## A sample text in Arabic for demo - This sentence will be translated to English"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"اليوم سنناقش الطقس في دبي\"\"\"\n",
        "#testing w ar\n",
        "#This senstce is lets discuss the weather in dubai"
      ],
      "metadata": {
        "id": "o8uCKdc5oTI1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XftYgju4XOw_"
      },
      "source": [
        "## Define NLP pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Assembler = DocumentAssembler()\\\n",
        "  .setInputCol(\"text\")\\\n",
        "  .setOutputCol(\"document\")\n",
        "\n",
        "#this adds a deep learning model that is used to detect edges or boundaries of\n",
        "#a sentence (it makes it more accurate)\n",
        "DL_model = SentenceDetectorDLModel()\\\n",
        "  .pretrained(\"sentence_detector_dl\", \"xx\")\\\n",
        "  .setInputCols([\"document\"])\\\n",
        "  .setOutputCol(\"sentences\")\n",
        "#loading transformer\n",
        "marian_t = MarianTransformer.pretrained(\"opus_mt_ar_en\", \"xx\")\\\n",
        "  .setInputCols([\"sentences\"])\\\n",
        "  .setOutputCol(\"translation\")\n",
        "#initalizing the pipeline\n",
        "pipeline_init = Pipeline(\n",
        "    stages=[\n",
        "        Assembler,\n",
        "        DL_model,\n",
        "        marian_t\n",
        "        ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLpLkZxLocLp",
        "outputId": "b106e5f9-ab50-4b75-982f-5c6dbcd1cd54"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 514.9 KB\n",
            "[OK!]\n",
            "opus_mt_ar_en download started this may take some time.\n",
            "Approximate size to download 390.7 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv0abcwhXWC-"
      },
      "source": [
        "## Run the pipeline To start translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EYf_9sXDXR4t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "338e6e74-fa3c-4797-f783-f507d8e2ece2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before _validateStagesInputCols\n"
          ]
        }
      ],
      "source": [
        "df = spark_instance.createDataFrame([[\"\"]]).toDF('text')\n",
        "model = pipeline_init.fit(df)\n",
        "lmodel = LightPipeline(model) #this is light pipeline bc it makes it run\n",
        "#more effectively\n",
        "res = lmodel.fullAnnotate(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQY8tAP6XZJL"
      },
      "source": [
        "## Results for Terminal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ar32BZu7J79X",
        "outputId": "8c6ac345-44ba-4c98-c545-be8f11a3c110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: اليوم سنناقش الطقس في دبي \n",
            "\n",
            "\n",
            "Translated:\n",
            "\n",
            "Today we're discussing the weather in Dubai.\n"
          ]
        }
      ],
      "source": [
        "print ('Original:', text, '\\n\\n')\n",
        "\n",
        "print ('Translated:\\n')\n",
        "for sentence in res[0]['translation']:\n",
        "  print (sentence.result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploying the Spark Based Model using Flask and ngrok"
      ],
      "metadata": {
        "id": "3B84uIY8tZ61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Flask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVrkzGj2tpV9",
        "outputId": "8b155145-5e0b-4a46-f1f7-898c554f329f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask) (3.0.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW9OKX93-RLg",
        "outputId": "6c6ac006-8607-46a7-f862-a19cbe93e4a0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Flask pyngrok\n",
        "!pip install -q pyspark==3.3.0 spark-nlp==4.2.8\n",
        "!pip install --upgrade -q spark-nlp-display"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9JKBNsn-8r4",
        "outputId": "9cab008e-bf09-414d-9937-bf54e2a5849f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.1.6)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask) (3.0.3)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (8.1.7)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sparknlp\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "from sparknlp.annotator import DocumentAssembler, SentenceDetectorDLModel, MarianTransformer\n",
        "from sparknlp.base import LightPipeline\n",
        "from flask import Flask, request, render_template_string"
      ],
      "metadata": {
        "id": "j3TIt9W7--bR"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spark instance\n",
        "spark = sparknlp.start()\n",
        "\n",
        "# initializing Pipeline\n",
        "def get_pipeline():\n",
        "    documentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
        "    sentencerDL = SentenceDetectorDLModel().pretrained(\"sentence_detector_dl\", \"xx\").setInputCols([\"document\"]).setOutputCol(\"sentences\")\n",
        "    marian = MarianTransformer.pretrained(\"opus_mt_ar_en\", \"xx\").setInputCols([\"sentences\"]).setOutputCol(\"translation\")\n",
        "\n",
        "    nlp_pipeline = Pipeline(stages=[documentAssembler, sentencerDL, marian])\n",
        "    empty_df = spark.createDataFrame([[\"\"]]).toDF('text')\n",
        "    pipeline_model = nlp_pipeline.fit(empty_df)\n",
        "    lmodel = LightPipeline(pipeline_model)\n",
        "    return lmodel\n",
        "\n",
        "# this will load the above pipeline to do MT ar ==> eng\n",
        "pipeline = get_pipeline()\n",
        "\n",
        "# Starting Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Colab cannot take HTML files as is so declare as a string and then use\n",
        "template = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <title>Machine Translation</title>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Machine Translation</h1>\n",
        "    <form method=\"POST\" action=\"/translate\">\n",
        "        <label for=\"text\">Text to translate:</label><br>\n",
        "        <textarea id=\"text\" name=\"text\" rows=\"4\" cols=\"50\"></textarea><br><br>\n",
        "\n",
        "        <input type=\"submit\" value=\"Translate\">\n",
        "    </form>\n",
        "    {% if original_text %}\n",
        "        <h2>Original Text:</h2>\n",
        "        <p>{{ original_text }}</p>\n",
        "        <h2>Translated Text:</h2>\n",
        "        <p>{{ translated_text }}</p>\n",
        "    {% endif %}\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template_string(template)\n",
        "\n",
        "@app.route('/translate', methods=['POST'])\n",
        "def translate():\n",
        "    text = request.form['text']\n",
        "    res = pipeline.fullAnnotate(text)\n",
        "\n",
        "    translated_text = \" \".join([sentence.result for sentence in res[0]['translation']])\n",
        "    return render_template_string(template, original_text=text, translated_text=translated_text)\n",
        "\n",
        "# Run Flask app\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Set  ngrok authtoken\n",
        "NGROK_AUTH_TOKEN = \"2NpCES1cJyv7FsZtPHzY7EqbHlZ_25u5HZPeGmjdwu1CeFLZ1\"\n",
        "#this token is from my uni account\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f'Public URL: {public_url}')\n",
        "app.run(port=5000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfO_1qGntrMW",
        "outputId": "08377fe9-e8db-4e85-b0a3-a0b75e9a8769"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 514.9 KB\n",
            "[OK!]\n",
            "opus_mt_ar_en download started this may take some time.\n",
            "Approximate size to download 390.7 MB\n",
            "[OK!]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-06-11T17:38:36+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://2d72-34-80-14-48.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#better HTML\n",
        "# spark instance\n",
        "spark = sparknlp.start()\n",
        "\n",
        "# initializing Pipeline\n",
        "def get_pipeline():\n",
        "    documentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
        "    sentencerDL = SentenceDetectorDLModel().pretrained(\"sentence_detector_dl\", \"xx\").setInputCols([\"document\"]).setOutputCol(\"sentences\")\n",
        "    marian = MarianTransformer.pretrained(\"opus_mt_ar_en\", \"xx\").setInputCols([\"sentences\"]).setOutputCol(\"translation\")\n",
        "\n",
        "    nlp_pipeline = Pipeline(stages=[documentAssembler, sentencerDL, marian])\n",
        "    empty_df = spark.createDataFrame([[\"\"]]).toDF('text')\n",
        "    pipeline_model = nlp_pipeline.fit(empty_df)\n",
        "    lmodel = LightPipeline(pipeline_model)\n",
        "    return lmodel\n",
        "\n",
        "# this will load the above pipeline to do MT ar ==> eng\n",
        "pipeline = get_pipeline()\n",
        "\n",
        "# Starting Flask app\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Colab cannot take HTML files as is so declare as a string and then use\n",
        "template = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <title>Machine Translation</title>\n",
        "    <style>\n",
        "        body {\n",
        "            background-color: #f3e5f5;\n",
        "            color: #4a148c;\n",
        "            font-family: Arial, sans-serif;\n",
        "            text-align: center;\n",
        "            margin: 0;\n",
        "            padding: 0;\n",
        "        }\n",
        "\n",
        "        h1 {\n",
        "            font-size: 2.5em;\n",
        "            margin: 20px 0;\n",
        "            color: #4a148c;\n",
        "        }\n",
        "\n",
        "        p {\n",
        "            font-size: 1.2em;\n",
        "            margin: 10px 0;\n",
        "            color: #6a1b9a;\n",
        "            text-align: center;\n",
        "        }\n",
        "\n",
        "        form {\n",
        "            display: inline-block;\n",
        "            margin-top: 20px;\n",
        "        }\n",
        "\n",
        "        label {\n",
        "            font-size: 1.2em;\n",
        "        }\n",
        "\n",
        "        textarea {\n",
        "            width: 50%;\n",
        "            height: 100px;\n",
        "            border: 2px solid #4a148c;\n",
        "            border-radius: 10px;\n",
        "            padding: 10px;\n",
        "            font-size: 1em;\n",
        "            margin-top: 10px;\n",
        "            margin-bottom: 20px;\n",
        "        }\n",
        "\n",
        "        input[type=\"submit\"] {\n",
        "            background-color: #7b1fa2;\n",
        "            color: white;\n",
        "            padding: 10px 20px;\n",
        "            border: none;\n",
        "            border-radius: 20px;\n",
        "            font-size: 1.2em;\n",
        "            cursor: pointer;\n",
        "        }\n",
        "\n",
        "        input[type=\"submit\"]:hover {\n",
        "            background-color: #6a1b9a;\n",
        "        }\n",
        "\n",
        "        h2 {\n",
        "            font-size: 1.8em;\n",
        "            margin: 20px 0 10px;\n",
        "            color: #4a148c;\n",
        "        }\n",
        "\n",
        "        textarea, p, input[type=\"submit\"], h2 {\n",
        "            text-align: left;\n",
        "            display: block;\n",
        "            margin-left: auto;\n",
        "            margin-right: auto;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Machine Translation</h1>\n",
        "    <p>Enter the text you want to translate from Arabic to English in the box below and press the translate button.</p>\n",
        "    <form method=\"POST\" action=\"/translate\">\n",
        "        <label for=\"text\">Text to translate:</label><br>\n",
        "        <textarea id=\"text\" name=\"text\" rows=\"4\" cols=\"50\"></textarea><br><br>\n",
        "        <input type=\"submit\" value=\"Translate\">\n",
        "    </form>\n",
        "    {% if original_text %}\n",
        "        <h2>Original Text:</h2>\n",
        "        <p>{{ original_text }}</p>\n",
        "        <h2>Translated Text:</h2>\n",
        "        <p>{{ translated_text }}</p>\n",
        "    {% endif %}\n",
        "</body>\n",
        "</html>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template_string(template)\n",
        "\n",
        "@app.route('/translate', methods=['POST'])\n",
        "def translate():\n",
        "    text = request.form['text']\n",
        "    res = pipeline.fullAnnotate(text)\n",
        "\n",
        "    translated_text = \" \".join([sentence.result for sentence in res[0]['translation']])\n",
        "    return render_template_string(template, original_text=text, translated_text=translated_text)\n",
        "\n",
        "# Run Flask app\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Set  ngrok authtoken\n",
        "NGROK_AUTH_TOKEN = \"2NpCES1cJyv7FsZtPHzY7EqbHlZ_25u5HZPeGmjdwu1CeFLZ1\"\n",
        "#this token is from my uni account\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f'Public URL: {public_url}')\n",
        "app.run(port=5000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXM5rUpHY-XO",
        "outputId": "a865ab03-5335-4754-cc6b-1c61c4e34bd4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 514.9 KB\n",
            "[OK!]\n",
            "opus_mt_ar_en download started this may take some time.\n",
            "Approximate size to download 390.7 MB\n",
            "[OK!]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-06-11T17:47:47+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://e15d-34-80-14-48.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Jun/2024 17:47:55] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [11/Jun/2024 17:47:56] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before _validateStagesInputCols\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [11/Jun/2024 17:48:25] \"POST /translate HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BLEU Scores"
      ],
      "metadata": {
        "id": "XOFTI4y1aeUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilHJmyRpeyKC",
        "outputId": "f8b13667-0811-4123-e260-b3ca239af1a8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "2jJ5wIVh0jG4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iT0Utc3Je1_l",
        "outputId": "243d0aff-ad30-411d-df48-dd0182a0ecdf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sparknlp\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "from sparknlp.annotator import DocumentAssembler, SentenceDetectorDLModel, MarianTransformer\n",
        "from sparknlp.base import LightPipeline\n",
        "from flask import Flask, request, render_template_string"
      ],
      "metadata": {
        "id": "oaZ7BbQIc2vH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "def normalize_and_tokenize(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# testing on same sample as before\n",
        "text = \"اليوم سنناقش الطقس في دبي\"\n",
        "\n",
        "# same components as before for setup\n",
        "Assembler = DocumentAssembler()\\\n",
        "  .setInputCol(\"text\")\\\n",
        "  .setOutputCol(\"document\")\n",
        "\n",
        "DL_model = SentenceDetectorDLModel()\\\n",
        "  .pretrained(\"sentence_detector_dl\", \"xx\")\\\n",
        "  .setInputCols([\"document\"])\\\n",
        "  .setOutputCol(\"sentences\")\n",
        "\n",
        "marian_t = MarianTransformer.pretrained(\"opus_mt_ar_en\", \"xx\")\\\n",
        "  .setInputCols([\"sentences\"])\\\n",
        "  .setOutputCol(\"translation\")\n",
        "\n",
        "nlp_pipeline = Pipeline(stages=[Assembler, DL_model, marian_t])\n",
        "empty_df = spark.createDataFrame([[\"\"]]).toDF('text')\n",
        "pipeline_model = nlp_pipeline.fit(empty_df)\n",
        "lmodel = LightPipeline(pipeline_model)\n",
        "\n",
        "# Translate the text\n",
        "res = lmodel.fullAnnotate(text)\n",
        "translated_text = \" \".join([sentence.result for sentence in res[0]['translation']])\n",
        "print('Original:', text, '\\n\\n')\n",
        "print('Translated:', translated_text, '\\n')\n",
        "\n",
        "# Reference translation (for demonstration purposes)\n",
        "reference_translation = [\"Today we will discuss the weather in Dubai\"]\n",
        "\n",
        "# Normalize and tokenize\n",
        "reference_tokenized = [normalize_and_tokenize(ref) for ref in reference_translation][0]  # Flatten list\n",
        "translated_tokenized = normalize_and_tokenize(translated_text)\n",
        "\n",
        "# Calculate BLEU score\n",
        "individual_bleu_score = sentence_bleu([reference_tokenized], translated_tokenized)\n",
        "cumulative_bleu_score = corpus_bleu([[reference_tokenized]], [translated_tokenized])\n",
        "\n",
        "# Print BLEU scores\n",
        "print(f\"Individual BLEU score: {individual_bleu_score}\")\n",
        "print(f\"Cumulative BLEU score: {cumulative_bleu_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sO-i539helnR",
        "outputId": "9af42f8e-0b0f-476e-8d70-d870c9e2dbca"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 514.9 KB\n",
            "[OK!]\n",
            "opus_mt_ar_en download started this may take some time.\n",
            "Approximate size to download 390.7 MB\n",
            "[OK!]\n",
            "Before _validateStagesInputCols\n",
            "Original: اليوم سنناقش الطقس في دبي \n",
            "\n",
            "\n",
            "Translated: Today we're discussing the weather in Dubai. \n",
            "\n",
            "Individual BLEU score: 0.3768499164492419\n",
            "Cumulative BLEU score: 0.3768499164492419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BLEU score ==> closer value to 1 the better our score = 0.4"
      ],
      "metadata": {
        "id": "2bLcJd2t1XiK"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "interpreter": {
      "hash": "45150093197569bb3a58481dcd32cd1adb45462fa3448719e8ac38ada6166aca"
    },
    "kernelspec": {
      "display_name": "Python 3.6.10 64-bit ('tensorflow2_p36': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}